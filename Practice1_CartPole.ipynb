{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"collapsed_sections":["mBNj56Ii1VTw"],"authorship_tag":"ABX9TyOVWSC82w7vKU+ZYwKUiBFz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Cartpole 문제에 강화학습 적용하기\n","Reference: https://stable-baselines.readthedocs.io/en/master/guide/examples.html#try-it-online-with-colab-notebooks"],"metadata":{"id":"CN093jB6QvW8"}},{"cell_type":"markdown","source":["1. Stable-baselines3 설치\n","\n","\n","*   [extra] 옵션: Tensorboard, OpenCV, atari-py와 같이 경우에 따라 필요할 수 있는 package까지 설치\n","\n"],"metadata":{"id":"RQWbKPlnQ34B"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"71YhM4AuOzYR"},"outputs":[],"source":["!pip install stable-baselines3[extra]"]},{"cell_type":"code","source":["!pip install gym pyvirtualdisplay\n","!apt-get install -y xvfb python-opengl ffmpeg  # 시각화를 위해 설치\n","!pip install pyglet==1.5.26\n","!sudo apt-get install xvfb"],"metadata":{"id":"gbQ5YowrSYWR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. 설치된 stable_baselines3 불러오기"],"metadata":{"id":"gQmydWYHPDQr"}},{"cell_type":"code","source":["import stable_baselines3\n","\n","# 비디오 출력 위한 클래스 추가 불러오기\n","from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv"],"metadata":{"id":"SsBgbsvESv1n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3. stable_baselines3에서 사용 할 강화학습 알고리즘 (DQN) 불러오기"],"metadata":{"id":"sfY-_6w3XL1-"}},{"cell_type":"code","source":["from stable_baselines3 import DQN"],"metadata":{"id":"Sloi829-XL_-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["4. openAI gym의 강화학습 환경을 사용하기 위해 gym 불러오기"],"metadata":{"id":"lIAA4ablS9-3"}},{"cell_type":"code","source":["import gymnasium as gym"],"metadata":{"id":"22eZVl_QS9vP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["5. gym을 통해 cartpole 환경 생성하기"],"metadata":{"id":"seEwt5eATayA"}},{"cell_type":"code","source":["cartpole_env = gym.make('CartPole-v1')"],"metadata":{"id":"nHolN1W3TzCZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["6. Cartpole 환경에 대한 정보 확인\n","  * 2 종류의 액션: (좌, 우)\n","  * 4 차원의 연속적인 상태 공간:\n","    * 카트의 위치: -4.8 ~ 4.8\n","    * 카트의 속력: -Inf ~ Inf\n","    * 막대기의 각도: -0.418 rad (-24도) ~ 0.418 rad (24도)\n","    * 막대기 끝 부분의 속도: -Inf, Inf\n","  * 한 에피소드 당 최대 길이: 500"],"metadata":{"id":"B4VwSbA2dVKT"}},{"cell_type":"code","source":["print(\"Action space: \", cartpole_env.action_space)\n","print(\"Observation space: \", cartpole_env.observation_space)\n","print(\"Maximum episode steps: \", cartpole_env.spec.max_episode_steps)"],"metadata":{"id":"Z-qkjWbraj5E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["7. DQN 에이전트 (DQN 강화학습 모델)을 hyperparameter를 지정하여 생성하기\n","* Cart-pole 환경은 이미지 데이터를 사용하는 환경이 아니기 때문에 CNN이 아닌 fully-connected layer ('MlpPolicy') 사용\n","* 다른 hyperparameter들은 stable-baselines3 zoo에서 제공하는 값을 참고하여 사용 (https://github.com/DLR-RM/rl-baselines3-zoo)"],"metadata":{"id":"1tJwqHkpVKoI"}},{"cell_type":"code","source":["hyper_param = {'n_timesteps': 5e4,\n","               'policy': 'MlpPolicy',\n","               'learning_rate': 2.3e-3,\n","               'batch_size': 64, # MSE 계산에 사용 할 샘플 개수\n","               'buffer_size': 100000, # replay buffer에 저장 할 샘플 (s,a,r,s') 수\n","               'learning_starts': 1000,\n","               'gamma': 0.99,\n","               'target_update_interval': 10, # Target network에 학습된 parameter를 복사해 넣는 주기\n","               'train_freq': 256, # Q-network를 몇 스텝마다 학습할 것인지 c.f.) 원래 DQN은 1 step\n","               'gradient_steps': 128, # MSE 계산을 몇 번 할 것인지. c.f.) 원래 DQN은 1번\n","               'exploration_fraction': 0.16, # 전체 학습 기간 중 탐색을 위해 큰 epsilon 값을 감소시키는 기간. 이 기간 이후로는 수렴된 epsilon 값 사용\n","               'exploration_final_eps': 0.04, # 고정적으로 사용할 epsilon 값\n","               'policy_kwargs': dict(net_arch=[256, 256]),\n","               'verbose': 1 # 학습 진행 상황 출력하고 싶을 때 입력\n","               }"],"metadata":{"id":"F3gkNoP-VK9u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dqn_model = DQN(policy = hyper_param['policy'],\n","                env = cartpole_env,\n","                learning_rate = hyper_param['learning_rate'],\n","                batch_size = hyper_param['batch_size'],\n","                buffer_size = hyper_param['buffer_size'],\n","                learning_starts = hyper_param['learning_starts'],\n","                gamma = hyper_param['gamma'],\n","                target_update_interval = hyper_param['target_update_interval'],\n","                train_freq = hyper_param['train_freq'],\n","                gradient_steps = hyper_param['gradient_steps'],\n","                exploration_fraction = hyper_param['exploration_fraction'],\n","                exploration_final_eps = hyper_param['exploration_final_eps'],\n","                verbose = hyper_param['verbose'],\n","                policy_kwargs = hyper_param['policy_kwargs']\n","                )"],"metadata":{"id":"tAMXDy6-VwER"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["8. 학습을 시작하기 전에 DQN 에이전트의 성능 확인하기\n","* stable-baselines의 evaluate_policy(RL 모델, 평가 환경, 평가에 사용할 에피소드 수) 함수 사용.\n","* Warning의 의미: Monitor 클래스를 통해 gym 환경의 에피소드 종료 시그널(done)이 실제 에피소드 종료를 의미하도록 만드는 것. 일부 환경에서는 학습을 위해 실제 종료 이전에 중간 구분 지점을 잡기도 하기 때문."],"metadata":{"id":"jkClrIeoZYH1"}},{"cell_type":"code","source":["from stable_baselines3.common.evaluation import evaluate_policy\n","eval_env = gym.make('CartPole-v1') # 성능 평가용 환경을 별도로 생성\n","mean_reward, std_reward = evaluate_policy(dqn_model, eval_env, n_eval_episodes=100) # episode 100개 생성하여 평가\n","print(\"에피소드 당 평균 보상:\", mean_reward, \"에피소드 당 보상의 표준편차: \", std_reward)"],"metadata":{"id":"Im060XkmV1DC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["9. DQN 에이전트 학습시키기"],"metadata":{"id":"ne4sYnpC1y-W"}},{"cell_type":"code","source":["dqn_model.learn(total_timesteps=hyper_param['n_timesteps'])"],"metadata":{"id":"gRf-nSV5muwH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["10. 학습된 결과 확인하기"],"metadata":{"id":"r2A6qlnr16Nm"}},{"cell_type":"code","source":["mean_reward, std_reward = evaluate_policy(dqn_model, eval_env, n_eval_episodes=100)\n","print(\"에피소드 당 평균 보상:\", mean_reward, \"에피소드 당 보상의 표준편차: \", std_reward)"],"metadata":{"id":"l8FHynlVm9UR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["실습: Proximal Policy Optimization (PPO) 알고리즘을 이용해서 cart-pole 문제 풀기"],"metadata":{"id":"WMpWgBkWcYTM"}},{"cell_type":"code","source":["from stable_baselines3 import PPO\n","\n","# 채워넣어보기 실습\n","# 1. Cart-pole 환경 만들기\n","\n"],"metadata":{"id":"uFuvDIZbKI9Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2. PPO 에이전트 만들기; policy 파라미터('MlpPolicy')와 학습할 환경만 넘겨주기\n","\n"],"metadata":{"id":"h95iCn659SPo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 3. 학습 시작! 총 100000 step만큼 진행.\n","\n"],"metadata":{"id":"rUxLRQaf9IyX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 학습 결과 확인하기\n","\n"],"metadata":{"id":"jW3BydK5-_TY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["11. Video를 통해 랜덤 에이전트와 학습된 에이전트가 cart-pole 환경에서 움직이는 것 비교하기\n","\n","주의할 점: 실행 전 아래로 내려가서 Colab에서 video 출력을 위한 코드 먼저 실행시키세요.\n"],"metadata":{"id":"XZ0Jt14b9D3B"}},{"cell_type":"code","source":["# 1. 랜덤 에이전트\n","test_env = DummyVecEnv([lambda: gym.make('CartPole-v1', render_mode=\"rgb_array\")])\n","video_folder = \"video/\"\n","video_length = 100\n","test_env = VecVideoRecorder(test_env, video_folder,\n","                       record_video_trigger=lambda x: x == 0, video_length=video_length,)\n","\n","observation = test_env.reset() # 환경 초기화. observation은 첫 관측값\n","cum_reward = 0 # 에피소드 끝날 때까지의 총 보상값\n","done = False # 에피소드 종료 조건 저장\n","while not done: # 에피소드 종료 때까지 실행\n","    test_env.render() # gym의 render 함수는 gym 환경을 시각화하기 위한 용도로 사용\n","\n","    action = [test_env.action_space.sample()] # 환경명.action_space.sample()은 가능한 액션을 랜덤하게 하나 선택\n","    observation, reward, done, info = test_env.step(action)  # step 함수를 통해 선택된 액션을 수행하고, (다음 관측값, 보상, 에피소드 종료 여부, 부가적인 정보)를 얻음\n","    cum_reward += reward\n","\n","print(\"이번 에피소드의 총 보상값: \", cum_reward)\n","test_env.close()\n","show_video() # colab에서 저장된 video 출력하기 위한 함수"],"metadata":{"id":"_tcd24fkAM7W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2. 학습된 에이전트 (PPO)\n","test_env = DummyVecEnv([lambda: gym.make('CartPole-v1', render_mode=\"rgb_array\")])\n","video_folder = \"video/\"\n","video_length = 100\n","test_env = VecVideoRecorder(test_env, video_folder,\n","                       record_video_trigger=lambda x: x == 0, video_length=video_length,)\n","\n","observation = test_env.reset() # 환경 초기화. observation은 첫 관측값\n","cum_reward = 0 # 에피소드 끝날 때까지의 총 보상값\n","done = False # 에피소드 종료 조건 저장\n","while not done: # 에피소드 종료 때까지 실행\n","    test_env.render() # gym의 render 함수는 gym 환경을 시각화하기 위한 용도로 사용\n","\n","    action, _ = ppo_model.predict(observation) # 학습된 에이전트로 부터 액션 선택\n","    observation, reward, done, info = test_env.step(action)  # step 함수를 통해 선택된 액션을 수행하고, (다음 관측값, 보상, 에피소드 종료 여부, 부가적인 정보)를 얻음\n","    cum_reward += reward\n","\n","print(\"이번 에피소드의 총 보상값: \", cum_reward)\n","test_env.close()\n","show_video() # colab에서 저장된 video 출력하기 위한 함수"],"metadata":{"id":"gfVZENCx8oiI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Colab에서 openAI gym의 render 함수 이용해 video 출력을 위한 코드"],"metadata":{"id":"mBNj56Ii1VTw"}},{"cell_type":"code","source":["from pyvirtualdisplay import Display\n","display = Display(visible=0, size=(1400, 900))\n","display.start()"],"metadata":{"id":"-oqq-cRNAIUI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Utility functions to enable video recording of gym environment and displaying it\n","To enable video, just do \"env = wrap_env(env)\"\"\n","Reference: https://colab.research.google.com/drive/1flu31ulJlgiRL1dnN2ir8wGh9p7Zij2t#scrollTo=8nj5sjsk15IT\n","\"\"\"\n","\n","# from gym.wrappers import Monitor\n","import glob\n","import io\n","import base64\n","from IPython.display import HTML\n","from IPython import display as ipythondisplay\n","\n","def show_video():\n","  mp4list = glob.glob('video/*.mp4')\n","  if len(mp4list) > 0:\n","    mp4 = mp4list[0]\n","    video = io.open(mp4, 'r+b').read()\n","    encoded = base64.b64encode(video)\n","    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay\n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","             </video>'''.format(encoded.decode('ascii'))))\n","  else:\n","    print(\"Could not find video\")\n","\n","\n","# def wrap_env(env):\n","#   env = Monitor(env, './video', force=True)\n","#   return env"],"metadata":{"id":"Of86eoVfkMOi"},"execution_count":null,"outputs":[]}]}